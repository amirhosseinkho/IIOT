package test;

import algorithms.*;
import core.*;
import simulation.*;
import evaluation.*;

import java.util.*;
import java.io.*;

/**
 * Advanced Scenario Tester - Stress Testing for IIoT Scheduling Algorithms
 * 
 * Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ú†Ø§Ù„Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ø³Ø®Øª
 */
public class AdvancedScenarioTester {
    
    private static final String RESULTS_DIR = "stress_test_results/";
    private static final int STRESS_RUNS = 10; // ØªØ¹Ø¯Ø§Ø¯ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø± ØªØ³Øª
    
    public static void main(String[] args) {
        System.out.println("ğŸ”¥ğŸ”¥ğŸ”¥ ADVANCED STRESS TESTING FRAMEWORK ğŸ”¥ğŸ”¥ğŸ”¥");
        System.out.println("========================================================");
        System.out.println("Testing Enhanced EPO-CEIS under EXTREME conditions");
        System.out.println("========================================================\n");
        
        // Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ù†ØªØ§ÛŒØ¬
        new File(RESULTS_DIR).mkdirs();
        
        try {
            // Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ú†Ø§Ù„Ø´ÛŒ
            runAllStressTests();
            
            System.out.println("\nğŸ‰ All stress tests completed!");
            System.out.println("ğŸ“Š Results saved to: " + RESULTS_DIR);
            
        } catch (Exception e) {
            System.err.println("âŒ Stress testing failed: " + e.getMessage());
            e.printStackTrace();
        }
    }
    
    private static void runAllStressTests() {
        System.out.println("ğŸš€ Starting comprehensive stress testing...\n");
        
        // 1. ØªØ³Øª Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ
        runScalabilityStressTest();
        
        // 2. ØªØ³Øª deadline Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ù…Ú©Ù†
        runImpossibleDeadlineTest();
        
        // 3. ØªØ³Øª Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ù…Ù†Ø§Ø¨Ø¹
        runResourceImbalanceTest();
        
        // 4. ØªØ³Øª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø± ØªØ£Ø®ÛŒØ±
        runHighLatencyNetworkTest();
        
        // 5. ØªØ³Øª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø´Ø¯ÛŒØ¯ Ø§Ù†Ø±Ú˜ÛŒ
        runEnergyConstrainedTest();
        
        // 6. ØªØ³Øª ØªÙˆÙ¾ÙˆÙ„ÙˆÚ˜ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
        runComplexTopologyTest();
        
        // 7. ØªØ³Øª dynamic workload
        runDynamicWorkloadTest();
        
        // 8. ØªØ³Øª fault tolerance
        runFaultToleranceTest();
        
        // 9. ØªØ³Øª multi-objective conflicts
        runConflictingObjectivesTest();
        
        // 10. ØªØ³Øª extreme heterogeneity
        runExtremeHeterogeneityTest();
    }
    
    /**
     * 1. ØªØ³Øª Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ - Ø§Ø² 10 ØªØ§ 5000 ØªØ³Ú©
     */
    private static void runScalabilityStressTest() {
        System.out.println("ğŸ“ˆ SCALABILITY STRESS TEST");
        System.out.println("Testing algorithm performance with increasing workload sizes");
        System.out.println("==========================================================");
        
        int[] taskCounts = {10, 50, 100, 200, 500, 1000, 2000, 3000, 5000};
        
        for (int taskCount : taskCounts) {
            System.out.printf("\nğŸ”¬ Testing with %d tasks...\n", taskCount);
            
            // Ø§ÛŒØ¬Ø§Ø¯ workflow Ø¨Ø²Ø±Ú¯
            Workflow largeWorkflow = generateLargeWorkflow(taskCount);
            List<FogNode> nodes = generateScalableNodes(taskCount);
            
            long startTime = System.currentTimeMillis();
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(largeWorkflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                long executionTime = System.currentTimeMillis() - startTime;
                
                System.out.printf("  âœ… Success: Cost=%.2f, Time=%dms, Assignments=%d\n", 
                    result.totalCost, executionTime, result.taskAssignments.size());
                
                // Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
                saveScalabilityResult(taskCount, result, executionTime);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 2. ØªØ³Øª deadline Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ù…Ú©Ù†
     */
    private static void runImpossibleDeadlineTest() {
        System.out.println("\nâ° IMPOSSIBLE DEADLINE STRESS TEST");
        System.out.println("Testing with extremely tight and impossible deadlines");
        System.out.println("=====================================================");
        
        double[] deadlineFactors = {0.1, 0.3, 0.5, 0.7, 1.0, 1.5}; // factor of minimum possible time
        
        for (double factor : deadlineFactors) {
            System.out.printf("\nğŸ¯ Testing with deadline factor %.1f...\n", factor);
            
            Workflow workflow = generateTightDeadlineWorkflow(50, factor);
            List<FogNode> nodes = generateMixedNodes(15);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                // Ù…Ø­Ø§Ø³Ø¨Ù‡ deadline violations
                int violations = calculateDeadlineViolations(workflow, result);
                double hitRate = 1.0 - (double) violations / workflow.getAllTasks().size();
                
                System.out.printf("  ğŸ“Š Factor %.1f: Cost=%.2f, Hit Rate=%.1f%%, Violations=%d\n", 
                    factor, result.totalCost, hitRate * 100, violations);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 3. ØªØ³Øª Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ø´Ø¯ÛŒØ¯ Ù…Ù†Ø§Ø¨Ø¹
     */
    private static void runResourceImbalanceTest() {
        System.out.println("\nâš–ï¸ RESOURCE IMBALANCE STRESS TEST");
        System.out.println("Testing with severely imbalanced fog-cloud resources");
        System.out.println("===================================================");
        
        String[] scenarios = {
            "fog_dominant",    // 90% fog nodes, 10% cloud
            "cloud_dominant",  // 10% fog nodes, 90% cloud
            "few_powerful",    // Ú©Ù… ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ„ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯
            "many_weak",       // Ø²ÛŒØ§Ø¯ ÙˆÙ„ÛŒ Ø¶Ø¹ÛŒÙ
            "extreme_hetero"   // ØªÙØ§ÙˆØª Ø´Ø¯ÛŒØ¯ Ø¯Ø± Ù‚Ø¯Ø±Øª
        };
        
        for (String scenario : scenarios) {
            System.out.printf("\nğŸ—ï¸ Testing %s scenario...\n", scenario);
            
            Workflow workflow = generateMediumWorkflow(100);
            List<FogNode> nodes = generateImbalancedNodes(scenario, 20);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                // ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ ØªØ³Ú©â€ŒÙ‡Ø§
                analyzeTaskDistribution(result, nodes, scenario);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 4. ØªØ³Øª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø± ØªØ£Ø®ÛŒØ±
     */
    private static void runHighLatencyNetworkTest() {
        System.out.println("\nğŸŒ HIGH LATENCY NETWORK STRESS TEST");
        System.out.println("Testing with extremely high network latencies");
        System.out.println("===========================================");
        
        double[] latencyMultipliers = {1.0, 5.0, 10.0, 20.0, 50.0, 100.0};
        
        for (double multiplier : latencyMultipliers) {
            System.out.printf("\nğŸ“¡ Testing with latency multiplier %.1fx...\n", multiplier);
            
            Workflow workflow = generateDataIntensiveWorkflow(80);
            List<FogNode> nodes = generateHighLatencyNodes(12, multiplier);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                double avgLatency = calculateAverageLatency(result, workflow, nodes);
                
                System.out.printf("  ğŸ“Š Latency %.1fx: Cost=%.2f, Avg Latency=%.2fs\n", 
                    multiplier, result.totalCost, avgLatency);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 5. ØªØ³Øª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø´Ø¯ÛŒØ¯ Ø§Ù†Ø±Ú˜ÛŒ
     */
    private static void runEnergyConstrainedTest() {
        System.out.println("\nğŸ”‹ ENERGY CONSTRAINED STRESS TEST");
        System.out.println("Testing with severe energy limitations");
        System.out.println("====================================");
        
        double[] energyBudgets = {100.0, 200.0, 500.0, 1000.0, 2000.0}; // Wh
        
        for (double budget : energyBudgets) {
            System.out.printf("\nâš¡ Testing with energy budget %.0f Wh...\n", budget);
            
            Workflow workflow = generateEnergyIntensiveWorkflow(60);
            List<FogNode> nodes = generateEnergyAwareNodes(10);
            
            try {
                // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ù†Ø±Ú˜ÛŒ Ø¨Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                double totalEnergy = calculateTotalEnergyConsumption(result, workflow, nodes);
                
                System.out.printf("  ğŸ”‹ Budget %.0f Wh: Cost=%.2f, Energy Used=%.2f Wh, %s\n", 
                    budget, result.totalCost, totalEnergy, 
                    totalEnergy <= budget ? "âœ… Within Budget" : "âŒ Over Budget");
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 6. ØªØ³Øª ØªÙˆÙ¾ÙˆÙ„ÙˆÚ˜ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
     */
    private static void runComplexTopologyTest() {
        System.out.println("\nğŸ•¸ï¸ COMPLEX TOPOLOGY STRESS TEST");
        System.out.println("Testing with complex network topologies");
        System.out.println("=====================================");
        
        String[] topologies = {
            "star",           // ØªÙ…Ø§Ù… fog nodes Ø¨Ù‡ ÛŒÚ© cloud Ù…ØªØµÙ„
            "mesh",           // Ø§Ø±ØªØ¨Ø§Ø· Ú©Ø§Ù…Ù„
            "tree",           // Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø±Ø®ØªÛŒ
            "ring",           // Ø³Ø§Ø®ØªØ§Ø± Ø­Ù„Ù‚ÙˆÛŒ
            "random",         // Ø§ØªØµØ§Ù„Ø§Øª ØªØµØ§Ø¯ÙÛŒ
            "hierarchical"    // Ø³Ù„Ø³Ù„Ù‡ Ù…Ø±Ø§ØªØ¨ÛŒ Ú†Ù†Ø¯Ø³Ø·Ø­Ù‡
        };
        
        for (String topology : topologies) {
            System.out.printf("\nğŸŒ Testing %s topology...\n", topology);
            
            Workflow workflow = generateComplexWorkflow(70);
            List<FogNode> nodes = generateTopologyBasedNodes(topology, 15);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                analyzeTopologyPerformance(result, nodes, topology);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 7. ØªØ³Øª dynamic workload
     */
    private static void runDynamicWorkloadTest() {
        System.out.println("\nğŸ”„ DYNAMIC WORKLOAD STRESS TEST");
        System.out.println("Testing with time-varying workloads");
        System.out.println("==================================");
        
        int timeSteps = 10;
        
        for (int step = 0; step < timeSteps; step++) {
            System.out.printf("\nâ±ï¸ Time step %d/%d...\n", step + 1, timeSteps);
            
            // workload Ú©Ù‡ Ø¨Ø§ Ø²Ù…Ø§Ù† ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            Workflow dynamicWorkflow = generateDynamicWorkflow(step, 50 + step * 10);
            List<FogNode> nodes = generateDynamicNodes(step, 12);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(dynamicWorkflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                System.out.printf("  ğŸ“Š Step %d: Tasks=%d, Cost=%.2f\n", 
                    step + 1, dynamicWorkflow.getAllTasks().size(), result.totalCost);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 8. ØªØ³Øª fault tolerance
     */
    private static void runFaultToleranceTest() {
        System.out.println("\nğŸ› ï¸ FAULT TOLERANCE STRESS TEST");
        System.out.println("Testing with node failures and recovery");
        System.out.println("======================================");
        
        double[] failureRates = {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}; // Ø¯Ø±ØµØ¯ node Ù‡Ø§ÛŒÛŒ Ú©Ù‡ fail Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        
        for (double failureRate : failureRates) {
            System.out.printf("\nğŸ’¥ Testing with %.0f%% node failure rate...\n", failureRate * 100);
            
            Workflow workflow = generateFaultTolerantWorkflow(90);
            List<FogNode> originalNodes = generateReliableNodes(20);
            List<FogNode> workingNodes = simulateNodeFailures(originalNodes, failureRate);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, workingNodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                double adaptationScore = calculateAdaptationScore(result, originalNodes, workingNodes);
                
                System.out.printf("  ğŸ”§ Failure %.0f%%: Working Nodes=%d/%d, Cost=%.2f, Adaptation=%.2f\n", 
                    failureRate * 100, workingNodes.size(), originalNodes.size(), 
                    result.totalCost, adaptationScore);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 9. ØªØ³Øª multi-objective conflicts
     */
    private static void runConflictingObjectivesTest() {
        System.out.println("\nâš”ï¸ CONFLICTING OBJECTIVES STRESS TEST");
        System.out.println("Testing with severely conflicting optimization goals");
        System.out.println("==================================================");
        
        String[] conflictScenarios = {
            "cost_vs_latency",     // Ú©Ù…â€ŒØªØ±ÛŒÙ† Ù‡Ø²ÛŒÙ†Ù‡ Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ú©Ù…â€ŒØªØ±ÛŒÙ† ØªØ£Ø®ÛŒØ±
            "energy_vs_speed",     // Ú©Ù…â€ŒØªØ±ÛŒÙ† Ø§Ù†Ø±Ú˜ÛŒ Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† Ø§Ø¬Ø±Ø§
            "reliability_vs_cost", // Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ú©Ù…â€ŒØªØ±ÛŒÙ† Ù‡Ø²ÛŒÙ†Ù‡
            "load_balance_vs_opt", // ØªÙˆØ²ÛŒØ¹ Ù…ØªØ¹Ø§Ø¯Ù„ Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            "all_conflicts"        // ØªÙ…Ø§Ù… Ø§Ù‡Ø¯Ø§Ù Ø¯Ø± ØªØ¶Ø§Ø¯
        };
        
        for (String scenario : conflictScenarios) {
            System.out.printf("\nâš–ï¸ Testing %s conflict...\n", scenario);
            
            Workflow workflow = generateConflictingWorkflow(scenario, 80);
            List<FogNode> nodes = generateConflictingNodes(scenario, 15);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                analyzeObjectiveConflicts(result, workflow, nodes, scenario);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    /**
     * 10. ØªØ³Øª extreme heterogeneity
     */
    private static void runExtremeHeterogeneityTest() {
        System.out.println("\nğŸŒˆ EXTREME HETEROGENEITY STRESS TEST");
        System.out.println("Testing with extremely diverse nodes and tasks");
        System.out.println("============================================");
        
        int[] diversityLevels = {2, 5, 10, 20, 50}; // ØªØ¹Ø¯Ø§Ø¯ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù nodes
        
        for (int diversity : diversityLevels) {
            System.out.printf("\nğŸ¨ Testing with %d different node types...\n", diversity);
            
            Workflow workflow = generateHeterogeneousWorkflow(100);
            List<FogNode> nodes = generateExtremelyDiverseNodes(diversity, 25);
            
            try {
                EnhancedEPOCEIS algorithm = new EnhancedEPOCEIS(workflow, nodes);
                EnhancedEPOCEIS.SchedulingResult result = algorithm.schedule();
                
                double diversityUtilization = calculateDiversityUtilization(result, nodes);
                
                System.out.printf("  ğŸŒˆ Diversity %d: Cost=%.2f, Utilization=%.2f%%\n", 
                    diversity, result.totalCost, diversityUtilization * 100);
                
            } catch (Exception e) {
                System.out.printf("  âŒ Failed: %s\n", e.getMessage());
            }
        }
    }
    
    // Helper methods for generating test scenarios
    
    private static Workflow generateLargeWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            // ØªØ³Ú©â€ŒÙ‡Ø§ Ø¨Ø§ workload Ùˆ deadline Ù…ØªÙ†ÙˆØ¹
            int length = 1000 + rand.nextInt(9000); // 1K-10K MI
            long fileSize = 1 + rand.nextInt(100); // 1-100 MB
            long outputSize = 1 + rand.nextInt(50); // 1-50 MB
            int pes = 1;
            double deadline = 10.0 + rand.nextDouble() * 40.0; // 10-50 seconds
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ
            if (i > 1 && rand.nextDouble() < 0.3) { // 30% Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateScalableNodes(int taskCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        // ØªØ¹Ø¯Ø§Ø¯ nodes Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ tasks
        int nodeCount = Math.max(5, taskCount / 20); // Ø­Ø¯Ø§Ù‚Ù„ 5ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± taskCount/20
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.7; // 30% cloud nodes
            
            int mips = isCloud ? 3000 + rand.nextInt(7000) : 500 + rand.nextInt(1500);
            int ram = isCloud ? 8192 : 1024 + rand.nextInt(3072);
            long bw = isCloud ? 1000 : 100 + rand.nextInt(400);
            long storage = isCloud ? 100000 : 10000 + rand.nextInt(40000);
            double costPerSec = isCloud ? 0.1 + rand.nextDouble() * 0.4 : 0.02 + rand.nextDouble() * 0.08;
            double latency = isCloud ? 50 + rand.nextDouble() * 100 : 5 + rand.nextDouble() * 20;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static Workflow generateTightDeadlineWorkflow(int taskCount, double deadlineFactor) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length = 2000 + rand.nextInt(8000);
            long fileSize = 10 + rand.nextInt(90);
            long outputSize = 5 + rand.nextInt(45);
            int pes = 1;
            
            // deadline Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¯Ø§Ù‚Ù„ Ø²Ù…Ø§Ù† Ù…Ù…Ú©Ù† Ø§Ø¬Ø±Ø§
            double minExecTime = length / 10000.0; // ÙØ±Ø¶ Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† node
            double deadline = minExecTime * deadlineFactor;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            // ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ
            if (i > 1 && rand.nextDouble() < 0.4) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateMixedNodes(int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.6;
            
            // ØªÙˆØ²ÛŒØ¹ Ù†Ø§Ù…ØªØ¬Ø§Ù†Ø³ Ù‚Ø¯Ø±Øª
            int mips, ram;
            long bw, storage;
            double costPerSec, latency;
            
            if (isCloud) {
                mips = 5000 + rand.nextInt(15000);
                ram = 16384;
                bw = 2000;
                storage = 200000;
                costPerSec = 0.2 + rand.nextDouble() * 0.6;
                latency = 80 + rand.nextDouble() * 120;
            } else {
                mips = 200 + rand.nextInt(1000); // fog nodes Ø¶Ø¹ÛŒÙâ€ŒØªØ±
                ram = 512 + rand.nextInt(1536);
                bw = 50 + rand.nextInt(200);
                storage = 5000 + rand.nextInt(15000);
                costPerSec = 0.01 + rand.nextDouble() * 0.05;
                latency = 2 + rand.nextDouble() * 15;
            }
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static Workflow generateMediumWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length = 1500 + rand.nextInt(3000);
            long fileSize = 5 + rand.nextInt(50);
            long outputSize = 2 + rand.nextInt(25);
            int pes = 1;
            double deadline = 15.0 + rand.nextDouble() * 25.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.25) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateImbalancedNodes(String scenario, int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        switch (scenario) {
            case "fog_dominant":
                // 90% fog, 10% cloud
                for (int i = 1; i <= nodeCount; i++) {
                    boolean isCloud = i > nodeCount * 0.9;
                    createNodeWithScenario(nodes, i, isCloud, rand, "normal");
                }
                break;
                
            case "cloud_dominant":
                // 10% fog, 90% cloud
                for (int i = 1; i <= nodeCount; i++) {
                    boolean isCloud = i > nodeCount * 0.1;
                    createNodeWithScenario(nodes, i, isCloud, rand, "normal");
                }
                break;
                
            case "few_powerful":
                // Ú©Ù… ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯
                for (int i = 1; i <= nodeCount; i++) {
                    boolean isCloud = rand.nextBoolean();
                    createNodeWithScenario(nodes, i, isCloud, rand, "powerful");
                }
                break;
                
            case "many_weak":
                // Ø²ÛŒØ§Ø¯ ÙˆÙ„ÛŒ Ù‡Ù…Ù‡ Ø¶Ø¹ÛŒÙ
                for (int i = 1; i <= nodeCount; i++) {
                    boolean isCloud = rand.nextBoolean();
                    createNodeWithScenario(nodes, i, isCloud, rand, "weak");
                }
                break;
                
            case "extreme_hetero":
                // ØªÙØ§ÙˆØª Ø´Ø¯ÛŒØ¯ Ø¯Ø± Ù‚Ø¯Ø±Øª
                for (int i = 1; i <= nodeCount; i++) {
                    boolean isCloud = rand.nextBoolean();
                    String type = rand.nextBoolean() ? "super_powerful" : "very_weak";
                    createNodeWithScenario(nodes, i, isCloud, rand, type);
                }
                break;
        }
        
        return nodes;
    }
    
    private static void createNodeWithScenario(List<FogNode> nodes, int id, boolean isCloud, Random rand, String type) {
        int mips, ram;
        long bw, storage;
        double costPerSec, latency;
        
        switch (type) {
            case "powerful":
                mips = isCloud ? 15000 + rand.nextInt(20000) : 2000 + rand.nextInt(5000);
                ram = isCloud ? 32768 : 8192;
                bw = isCloud ? 5000 : 1000;
                storage = isCloud ? 500000 : 100000;
                costPerSec = isCloud ? 0.5 + rand.nextDouble() : 0.1 + rand.nextDouble() * 0.2;
                latency = isCloud ? 30 + rand.nextDouble() * 50 : 1 + rand.nextDouble() * 5;
                break;
                
            case "weak":
                mips = isCloud ? 1000 + rand.nextInt(2000) : 100 + rand.nextInt(300);
                ram = isCloud ? 2048 : 256;
                bw = isCloud ? 100 : 10;
                storage = isCloud ? 10000 : 1000;
                costPerSec = isCloud ? 0.01 + rand.nextDouble() * 0.02 : 0.005 + rand.nextDouble() * 0.01;
                latency = isCloud ? 200 + rand.nextDouble() * 300 : 50 + rand.nextDouble() * 100;
                break;
                
            case "super_powerful":
                mips = 50000 + rand.nextInt(50000);
                ram = 65536;
                bw = 10000;
                storage = 1000000;
                costPerSec = 1.0 + rand.nextDouble() * 2.0;
                latency = 10 + rand.nextDouble() * 20;
                break;
                
            case "very_weak":
                mips = 50 + rand.nextInt(100);
                ram = 128;
                bw = 5;
                storage = 500;
                costPerSec = 0.001 + rand.nextDouble() * 0.002;
                latency = 500 + rand.nextDouble() * 1000;
                break;
                
            default: // normal
                mips = isCloud ? 3000 + rand.nextInt(7000) : 500 + rand.nextInt(1500);
                ram = isCloud ? 8192 : 1024;
                bw = isCloud ? 1000 : 100;
                storage = isCloud ? 100000 : 10000;
                costPerSec = isCloud ? 0.1 + rand.nextDouble() * 0.4 : 0.02 + rand.nextDouble() * 0.08;
                latency = isCloud ? 50 + rand.nextDouble() * 100 : 5 + rand.nextDouble() * 20;
                break;
        }
        
        FogNode node = new FogNode(id, mips, ram, bw, storage, isCloud, costPerSec, latency);
        nodes.add(node);
    }
    
    private static Workflow generateDataIntensiveWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length = 1000 + rand.nextInt(3000);
            // Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯
            long fileSize = 100 + rand.nextInt(900); // 100MB - 1GB
            long outputSize = 50 + rand.nextInt(450); // 50MB - 500MB
            int pes = 1;
            double deadline = 20.0 + rand.nextDouble() * 30.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            // ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ø¯Ù‡ Ø¨ÛŒØ´ØªØ±
            if (i > 1 && rand.nextDouble() < 0.5) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateHighLatencyNodes(int nodeCount, double latencyMultiplier) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.5;
            
            int mips = isCloud ? 4000 + rand.nextInt(6000) : 600 + rand.nextInt(1400);
            int ram = isCloud ? 8192 : 1024;
            long bw = isCloud ? 500 : 50; // bandwidth Ú©Ù… Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            long storage = isCloud ? 100000 : 10000;
            double costPerSec = isCloud ? 0.15 : 0.03;
            
            // latency Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯
            double baseLatency = isCloud ? 100 : 20;
            double latency = baseLatency * latencyMultiplier;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static Workflow generateEnergyIntensiveWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            // ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ú©Ù‡ Ø§Ù†Ø±Ú˜ÛŒ Ø²ÛŒØ§Ø¯ Ù…ØµØ±Ù Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
            int length = 5000 + rand.nextInt(15000); // heavy computation
            long fileSize = 20 + rand.nextInt(80);
            long outputSize = 10 + rand.nextInt(40);
            int pes = 1;
            double deadline = 30.0 + rand.nextDouble() * 60.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.3) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateEnergyAwareNodes(int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.6;
            
            int mips = isCloud ? 3000 + rand.nextInt(7000) : 500 + rand.nextInt(1500);
            int ram = isCloud ? 8192 : 1024;
            long bw = isCloud ? 1000 : 100;
            long storage = isCloud ? 100000 : 10000;
            double costPerSec = isCloud ? 0.1 + rand.nextDouble() * 0.4 : 0.02 + rand.nextDouble() * 0.08;
            double latency = isCloud ? 50 + rand.nextDouble() * 100 : 5 + rand.nextDouble() * 20;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            
            // ØªÙ†Ø¸ÛŒÙ… Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ
            double energyPerSec = isCloud ? 200 + rand.nextDouble() * 300 : 50 + rand.nextDouble() * 150;
            node.setEnergyPerSec(energyPerSec);
            
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static void saveScalabilityResult(int taskCount, EnhancedEPOCEIS.SchedulingResult result, long executionTime) {
        try {
            PrintWriter writer = new PrintWriter(new FileWriter(RESULTS_DIR + "scalability_results.csv", true));
            writer.printf("%d,%.2f,%d,%d\n", taskCount, result.totalCost, executionTime, result.taskAssignments.size());
            writer.close();
        } catch (IOException e) {
            System.err.println("Error saving scalability result: " + e.getMessage());
        }
    }
    
    private static int calculateDeadlineViolations(Workflow workflow, EnhancedEPOCEIS.SchedulingResult result) {
        int violations = 0;
        
        for (IIoTTask task : workflow.getAllTasks()) {
            if (result.startTimes.containsKey(task.getId())) {
                double startTime = result.startTimes.get(task.getId());
                double finishTime = startTime + task.getLength() / 1000.0; // approximate execution time
                
                if (finishTime > task.getDeadline()) {
                    violations++;
                }
            }
        }
        
        return violations;
    }
    
    private static void analyzeTaskDistribution(EnhancedEPOCEIS.SchedulingResult result, List<FogNode> nodes, String scenario) {
        // ØªØ­Ù„ÛŒÙ„ Ù†Ø­ÙˆÙ‡ ØªÙˆØ²ÛŒØ¹ ØªØ³Ú©â€ŒÙ‡Ø§ Ø±ÙˆÛŒ nodes Ù…Ø®ØªÙ„Ù
        Map<Integer, Integer> nodeTaskCount = new HashMap<>();
        
        for (Integer nodeId : result.taskAssignments.values()) {
            nodeTaskCount.merge(nodeId, 1, Integer::sum);
        }
        
        System.out.printf("    Task Distribution in %s:\n", scenario);
        for (FogNode node : nodes) {
            int taskCount = nodeTaskCount.getOrDefault(node.getId(), 0);
            System.out.printf("      Node %d (%s): %d tasks\n", 
                node.getId(), node.isCloud() ? "Cloud" : "Fog", taskCount);
        }
    }
    
    private static double calculateAverageLatency(EnhancedEPOCEIS.SchedulingResult result, Workflow workflow, List<FogNode> nodes) {
        double totalLatency = 0.0;
        int taskCount = 0;
        
        Map<Integer, FogNode> nodeMap = new HashMap<>();
        for (FogNode node : nodes) {
            nodeMap.put(node.getId(), node);
        }
        
        for (Map.Entry<Integer, Integer> entry : result.taskAssignments.entrySet()) {
            FogNode assignedNode = nodeMap.get(entry.getValue());
            if (assignedNode != null) {
                totalLatency += assignedNode.getLatencyMs();
                taskCount++;
            }
        }
        
        return taskCount > 0 ? totalLatency / taskCount : 0.0;
    }
    
    private static double calculateTotalEnergyConsumption(EnhancedEPOCEIS.SchedulingResult result, Workflow workflow, List<FogNode> nodes) {
        double totalEnergy = 0.0;
        
        Map<Integer, FogNode> nodeMap = new HashMap<>();
        for (FogNode node : nodes) {
            nodeMap.put(node.getId(), node);
        }
        
        for (IIoTTask task : workflow.getAllTasks()) {
            Integer nodeId = result.taskAssignments.get(task.getId());
            if (nodeId != null) {
                FogNode node = nodeMap.get(nodeId);
                if (node != null) {
                    double executionTime = (double) task.getLength() / node.getMips();
                    // ÙØ±Ø¶: Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ Ùˆ Ù‚Ø¯Ø±Øª Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±
                    double energy = executionTime * node.getMips() * 0.001; // ØªÙ‚Ø±ÛŒØ¨ÛŒ
                    totalEnergy += energy;
                }
            }
        }
        
        return totalEnergy;
    }
    
    // Helper methods for remaining test scenarios
    
    private static Workflow generateComplexWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length = 2000 + rand.nextInt(4000);
            long fileSize = 10 + rand.nextInt(70);
            long outputSize = 5 + rand.nextInt(35);
            int pes = 1;
            double deadline = 25.0 + rand.nextDouble() * 35.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            // Ø§ÛŒØ¬Ø§Ø¯ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
            if (i > 1) {
                int dependencyCount = 1 + rand.nextInt(Math.min(3, i - 1));
                Set<Integer> parents = new HashSet<>();
                
                for (int j = 0; j < dependencyCount; j++) {
                    int parentId = 1 + rand.nextInt(i - 1);
                    if (!parents.contains(parentId)) {
                        workflow.addDependency(parentId, i);
                        parents.add(parentId);
                    }
                }
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateTopologyBasedNodes(String topology, int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = determineCloudByTopology(topology, i, nodeCount);
            
            int mips = isCloud ? 4000 + rand.nextInt(6000) : 600 + rand.nextInt(1400);
            int ram = isCloud ? 8192 : 1024;
            long bw = calculateBandwidthByTopology(topology, i, nodeCount, rand);
            long storage = isCloud ? 100000 : 10000;
            double costPerSec = isCloud ? 0.12 : 0.025;
            double latency = calculateLatencyByTopology(topology, i, nodeCount, rand);
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static boolean determineCloudByTopology(String topology, int nodeId, int totalNodes) {
        switch (topology) {
            case "star":
                return nodeId == 1; // Ù…Ø±Ú©Ø² Ø³ØªØ§Ø±Ù‡ cloud Ø§Ø³Øª
            case "hierarchical":
                return nodeId <= totalNodes / 4; // 25% cloud nodes Ø¯Ø± Ø¨Ø§Ù„Ø§ÛŒ Ø³Ù„Ø³Ù„Ù‡ Ù…Ø±Ø§ØªØ¨
            default:
                return nodeId > totalNodes * 0.7; // 30% cloud nodes
        }
    }
    
    private static long calculateBandwidthByTopology(String topology, int nodeId, int totalNodes, Random rand) {
        switch (topology) {
            case "mesh":
                return 1000 + rand.nextInt(1000); // bandwidth Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ mesh
            case "ring":
                return 200 + rand.nextInt(300); // bandwidth Ù…ØªÙˆØ³Ø·
            case "tree":
                return nodeId <= totalNodes / 2 ? 800 : 300; // ÙˆØ§Ù„Ø¯ÛŒÙ† bandwidth Ø¨ÛŒØ´ØªØ±
            default:
                return 100 + rand.nextInt(400);
        }
    }
    
    private static double calculateLatencyByTopology(String topology, int nodeId, int totalNodes, Random rand) {
        switch (topology) {
            case "star":
                return nodeId == 1 ? 10 : 30 + rand.nextDouble() * 50; // Ù…Ø±Ú©Ø² latency Ú©Ù…
            case "mesh":
                return 5 + rand.nextDouble() * 15; // latency Ú©Ù… Ø¨Ø±Ø§ÛŒ mesh
            case "ring":
                return 20 + rand.nextDouble() * 60; // latency Ù…ØªØºÛŒØ±
            default:
                return 15 + rand.nextDouble() * 45;
        }
    }
    
    private static void analyzeTopologyPerformance(EnhancedEPOCEIS.SchedulingResult result, List<FogNode> nodes, String topology) {
        int fogTasks = 0, cloudTasks = 0;
        double totalLatency = 0.0;
        
        Map<Integer, FogNode> nodeMap = new HashMap<>();
        for (FogNode node : nodes) {
            nodeMap.put(node.getId(), node);
        }
        
        for (Integer nodeId : result.taskAssignments.values()) {
            FogNode node = nodeMap.get(nodeId);
            if (node != null) {
                if (node.isCloud()) {
                    cloudTasks++;
                } else {
                    fogTasks++;
                }
                totalLatency += node.getLatencyMs();
            }
        }
        
        double avgLatency = result.taskAssignments.size() > 0 ? 
            totalLatency / result.taskAssignments.size() : 0.0;
        
        System.out.printf("    %s Topology Analysis:\n", topology);
        System.out.printf("      Fog tasks: %d, Cloud tasks: %d\n", fogTasks, cloudTasks);
        System.out.printf("      Average latency: %.2f ms\n", avgLatency);
        System.out.printf("      Total cost: %.2f\n", result.totalCost);
    }
    
    private static Workflow generateDynamicWorkflow(int timeStep, int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random(timeStep * 1000); // seed Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ
        
        // ØªØºÛŒÛŒØ± Ø§Ù„Ú¯ÙˆÛŒ workload Ø¨Ø± Ø§Ø³Ø§Ø³ time step
        double loadFactor = 1.0 + 0.5 * Math.sin(timeStep * Math.PI / 5); // Ú†Ø±Ø®Ù‡â€ŒØ§ÛŒ
        
        for (int i = 1; i <= taskCount; i++) {
            int length = (int)(1500 * loadFactor) + rand.nextInt(2000);
            long fileSize = 8 + rand.nextInt(40);
            long outputSize = 4 + rand.nextInt(20);
            int pes = 1;
            
            // deadline Ø³Ø®Øªâ€ŒØªØ± Ø¯Ø± time steps Ø¨Ø§Ù„Ø§
            double deadlineBase = 20.0 - timeStep * 0.5;
            double deadline = Math.max(5.0, deadlineBase + rand.nextDouble() * 20.0);
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.3) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateDynamicNodes(int timeStep, int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random(timeStep * 2000);
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.6;
            
            // Ø¸Ø±ÙÛŒØª nodes Ø¨Ø§ Ø²Ù…Ø§Ù† ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ load balancing)
            double capacityFactor = 0.7 + 0.6 * Math.cos(timeStep * Math.PI / 3);
            
            int mips = (int)((isCloud ? 4000 : 800) * capacityFactor) + rand.nextInt(2000);
            int ram = isCloud ? 8192 : 1024;
            long bw = isCloud ? 1000 : 150;
            long storage = isCloud ? 100000 : 10000;
            
            // Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø§ Ø²Ù…Ø§Ù† ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (peak hours)
            double costMultiplier = 1.0 + 0.3 * Math.sin(timeStep * Math.PI / 4);
            double costPerSec = (isCloud ? 0.1 : 0.02) * costMultiplier;
            
            double latency = isCloud ? 60 + rand.nextDouble() * 80 : 8 + rand.nextDouble() * 25;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    // Helper methods for fault tolerance test
    
    private static Workflow generateFaultTolerantWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length = 1800 + rand.nextInt(3200);
            long fileSize = 12 + rand.nextInt(48);
            long outputSize = 6 + rand.nextInt(24);
            int pes = 1;
            double deadline = 18.0 + rand.nextDouble() * 32.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.35) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateReliableNodes(int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.65;
            
            int mips = isCloud ? 3500 + rand.nextInt(6500) : 550 + rand.nextInt(1450);
            int ram = isCloud ? 8192 : 1024;
            long bw = isCloud ? 1200 : 120;
            long storage = isCloud ? 120000 : 12000;
            double costPerSec = isCloud ? 0.11 : 0.022;
            double latency = isCloud ? 55 + rand.nextDouble() * 95 : 6 + rand.nextDouble() * 22;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static List<FogNode> simulateNodeFailures(List<FogNode> originalNodes, double failureRate) {
        List<FogNode> workingNodes = new ArrayList<>();
        Random rand = new Random();
        
        for (FogNode node : originalNodes) {
            if (rand.nextDouble() > failureRate) { // node Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                workingNodes.add(node);
            }
        }
        
        // Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© node
        if (workingNodes.isEmpty() && !originalNodes.isEmpty()) {
            workingNodes.add(originalNodes.get(0));
        }
        
        return workingNodes;
    }
    
    private static double calculateAdaptationScore(EnhancedEPOCEIS.SchedulingResult result, List<FogNode> originalNodes, List<FogNode> workingNodes) {
        double originalCapacity = originalNodes.stream().mapToInt(FogNode::getMips).sum();
        double workingCapacity = workingNodes.stream().mapToInt(FogNode::getMips).sum();
        
        double capacityRatio = workingCapacity / originalCapacity;
        
        // Ø§Ù…ØªÛŒØ§Ø² adaptation Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÛŒÙÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø§Ù‚ÛŒÙ…Ø§Ù†Ø¯Ù‡
        return capacityRatio * (1.0 - result.totalCost / 1000.0); // ÙØ±Ø¶: Ø­Ø¯Ø§Ú©Ø«Ø± cost 1000
    }
    
    // Helper methods for conflicting objectives test
    
    private static Workflow generateConflictingWorkflow(String scenario, int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            int length, pes;
            long fileSize, outputSize;
            double deadline;
            
            switch (scenario) {
                case "cost_vs_latency":
                    // ØªØ³Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø³Ø±Ø¹Øª Ø¯Ø§Ø±Ù†Ø¯ ÙˆÙ„ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ø±Ø²Ø§Ù† Ø¨Ø§Ø´Ù†Ø¯
                    length = 3000 + rand.nextInt(7000); // heavy tasks
                    fileSize = 50 + rand.nextInt(150);
                    outputSize = 25 + rand.nextInt(75);
                    deadline = 8.0 + rand.nextDouble() * 12.0; // tight deadlines
                    break;
                    
                case "energy_vs_speed":
                    // ØªØ³Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø³Ø±ÛŒØ¹ Ø¨Ø§ÛŒØ¯ Ø§Ø¬Ø±Ø§ Ø´ÙˆÙ†Ø¯ ÙˆÙ„ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ú©Ù… Ù…ØµØ±Ù Ú©Ù†Ù†Ø¯
                    length = 4000 + rand.nextInt(8000);
                    fileSize = 30 + rand.nextInt(100);
                    outputSize = 15 + rand.nextInt(50);
                    deadline = 5.0 + rand.nextDouble() * 10.0; // very tight
                    break;
                    
                case "reliability_vs_cost":
                    // ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø§Ø´Ù†Ø¯ ÙˆÙ„ÛŒ Ø§Ø±Ø²Ø§Ù†
                    length = 2000 + rand.nextInt(4000);
                    fileSize = 40 + rand.nextInt(120);
                    outputSize = 20 + rand.nextInt(60);
                    deadline = 15.0 + rand.nextDouble() * 25.0;
                    break;
                    
                default:
                    length = 2500 + rand.nextInt(5000);
                    fileSize = 20 + rand.nextInt(80);
                    outputSize = 10 + rand.nextInt(40);
                    deadline = 12.0 + rand.nextDouble() * 18.0;
                    break;
            }
            
            pes = 1;
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.4) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateConflictingNodes(String scenario, int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            boolean isCloud = i > nodeCount * 0.6;
            
            int mips;
            int ram;
            long bw, storage;
            double costPerSec, latency;
            
            switch (scenario) {
                case "cost_vs_latency":
                    // nodes Ø§Ø±Ø²Ø§Ù† ÙˆÙ„ÛŒ latency Ø²ÛŒØ§Ø¯ ÛŒØ§ Ú¯Ø±Ø§Ù† ÙˆÙ„ÛŒ Ø³Ø±ÛŒØ¹
                    if (rand.nextBoolean()) {
                        // cheap but slow
                        mips = isCloud ? 1000 + rand.nextInt(2000) : 200 + rand.nextInt(500);
                        costPerSec = 0.01 + rand.nextDouble() * 0.03;
                        latency = 100 + rand.nextDouble() * 200;
                    } else {
                        // fast but expensive
                        mips = isCloud ? 8000 + rand.nextInt(12000) : 1500 + rand.nextInt(2500);
                        costPerSec = 0.3 + rand.nextDouble() * 0.7;
                        latency = 5 + rand.nextDouble() * 15;
                    }
                    break;
                    
                case "energy_vs_speed":
                    // Ø³Ø±ÛŒØ¹ ÙˆÙ„ÛŒ Ù¾Ø± Ù…ØµØ±Ù ÛŒØ§ Ú©Ù†Ø¯ ÙˆÙ„ÛŒ Ú©Ù… Ù…ØµØ±Ù
                    if (rand.nextBoolean()) {
                        // fast but power hungry
                        mips = isCloud ? 10000 + rand.nextInt(15000) : 2000 + rand.nextInt(3000);
                        costPerSec = 0.2 + rand.nextDouble() * 0.5;
                        latency = 2 + rand.nextDouble() * 8;
                    } else {
                        // slow but energy efficient
                        mips = isCloud ? 1500 + rand.nextInt(2500) : 300 + rand.nextInt(700);
                        costPerSec = 0.02 + rand.nextDouble() * 0.05;
                        latency = 50 + rand.nextDouble() * 100;
                    }
                    break;
                    
                default:
                    mips = isCloud ? 3000 + rand.nextInt(7000) : 500 + rand.nextInt(1500);
                    costPerSec = isCloud ? 0.1 + rand.nextDouble() * 0.4 : 0.02 + rand.nextDouble() * 0.08;
                    latency = isCloud ? 50 + rand.nextDouble() * 100 : 5 + rand.nextDouble() * 20;
                    break;
            }
            
            ram = isCloud ? 8192 : 1024;
            bw = isCloud ? 1000 : 100;
            storage = isCloud ? 100000 : 10000;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static void analyzeObjectiveConflicts(EnhancedEPOCEIS.SchedulingResult result, Workflow workflow, List<FogNode> nodes, String scenario) {
        double totalCost = result.totalCost;
        double avgLatency = calculateAverageLatency(result, workflow, nodes);
        double totalEnergy = calculateTotalEnergyConsumption(result, workflow, nodes);
        
        System.out.printf("    %s Conflict Analysis:\n", scenario);
        System.out.printf("      Total Cost: %.2f\n", totalCost);
        System.out.printf("      Average Latency: %.2f ms\n", avgLatency);
        System.out.printf("      Energy Consumption: %.2f Wh\n", totalEnergy);
        
        // ØªØ­Ù„ÛŒÙ„ trade-off
        double costLatencyRatio = totalCost / Math.max(avgLatency, 1.0);
        double energySpeedRatio = totalEnergy / Math.max(avgLatency, 1.0);
        
        System.out.printf("      Cost/Latency Ratio: %.4f\n", costLatencyRatio);
        System.out.printf("      Energy/Speed Ratio: %.4f\n", energySpeedRatio);
    }
    
    // Helper methods for extreme heterogeneity test
    
    private static Workflow generateHeterogeneousWorkflow(int taskCount) {
        Workflow workflow = new Workflow();
        Random rand = new Random();
        
        for (int i = 1; i <= taskCount; i++) {
            // ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…ØªÙ†ÙˆØ¹
            int complexity = rand.nextInt(5); // 5 Ø³Ø·Ø­ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ
            
            int length = 500 + complexity * 2000 + rand.nextInt(1000);
            long fileSize = 5 + complexity * 20 + rand.nextInt(30);
            long outputSize = 2 + complexity * 10 + rand.nextInt(15);
            int pes = 1;
            double deadline = 10.0 + complexity * 8.0 + rand.nextDouble() * 20.0;
            
            IIoTTask task = new IIoTTask(i, length, fileSize, outputSize, pes, deadline);
            workflow.addTask(task);
            
            if (i > 1 && rand.nextDouble() < 0.3) {
                int parentId = 1 + rand.nextInt(i - 1);
                workflow.addDependency(parentId, i);
            }
        }
        
        return workflow;
    }
    
    private static List<FogNode> generateExtremelyDiverseNodes(int diversityLevel, int nodeCount) {
        List<FogNode> nodes = new ArrayList<>();
        Random rand = new Random();
        
        for (int i = 1; i <= nodeCount; i++) {
            // Ø§Ù†ØªØ®Ø§Ø¨ Ù†ÙˆØ¹ node Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù
            int nodeType = rand.nextInt(diversityLevel);
            boolean isCloud = rand.nextBoolean();
            
            // ØªØ¹Ø±ÛŒÙ Ø®ØµÙˆØµÛŒØ§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
            int baseMips = 100 + nodeType * 500;
            int mips = baseMips + rand.nextInt(baseMips);
            
            int baseRam = 256 + nodeType * 512;
            int ram = isCloud ? baseRam * 4 : baseRam;
            
            long baseBw = 10 + nodeType * 50;
            long bw = isCloud ? baseBw * 10 : baseBw;
            
            long baseStorage = 1000 + nodeType * 5000;
            long storage = isCloud ? baseStorage * 20 : baseStorage;
            
            double baseCost = 0.001 + nodeType * 0.02;
            double costPerSec = baseCost + rand.nextDouble() * baseCost;
            
            double baseLatency = 1 + nodeType * 10;
            double latency = baseLatency + rand.nextDouble() * baseLatency;
            
            FogNode node = new FogNode(i, mips, ram, bw, storage, isCloud, costPerSec, latency);
            nodes.add(node);
        }
        
        return nodes;
    }
    
    private static double calculateDiversityUtilization(EnhancedEPOCEIS.SchedulingResult result, List<FogNode> nodes) {
        Set<Integer> usedNodes = new HashSet<>(result.taskAssignments.values());
        
        // Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ†ÙˆØ¹ nodes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
        if (usedNodes.isEmpty()) return 0.0;
        
        double totalCapacity = 0.0;
        double usedCapacity = 0.0;
        
        Map<Integer, FogNode> nodeMap = new HashMap<>();
        for (FogNode node : nodes) {
            nodeMap.put(node.getId(), node);
            totalCapacity += node.getMips();
        }
        
        for (Integer nodeId : usedNodes) {
            FogNode node = nodeMap.get(nodeId);
            if (node != null) {
                usedCapacity += node.getMips();
            }
        }
        
        return usedCapacity / totalCapacity;
    }
}
